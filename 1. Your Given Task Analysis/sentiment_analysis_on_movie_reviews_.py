# -*- coding: utf-8 -*-
"""Sentiment Analysis on Movie Reviews .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uc0ct4LFEkfd7RANcllHu55XZDUyUH9F

# **Sentiment Analysis on Movie Reviews**
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
print("NLTK downloads completed")

"""# **Step 1: Data Preparation**"""

import pandas as pd
import re
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk

# Download NLTK resources
nltk.download('punkt')
nltk.download('punkt_tab')  # Required for newer NLTK versions
nltk.download('stopwords')
nltk.download('wordnet')

# Load the dataset
data = pd.read_csv('/content/drive/MyDrive/Project for Company/AIML Engineer position at Backbencher Studio/Dataset/archive/train_data.csv')

# Initialize stopwords and lemmatizer once (for speed)
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Clean the text
def clean_text(text):
    if pd.isnull(text):
        return ""
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    # Remove punctuation and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Convert to lowercase
    text = text.lower()
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords and lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Apply cleaning
data['cleaned_reviews'] = data['The Reviews'].apply(clean_text)

# Split into train and test sets
X = data['cleaned_reviews']
y = data['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display shape
print("Train size:", X_train.shape[0])
print("Test size:", X_test.shape[0])

"""# **Step 2: Model Training**"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# Create a pipeline with TF-IDF and Logistic Regression
model = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('clf', LogisticRegression(random_state=42))
])

# Train the model
model.fit(X_train, y_train)

"""# **Step 3: Evaluation**"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Predict on test set
y_pred = model.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Save the model
import joblib
joblib.dump(model, 'sentiment_model.pkl')

"""# **Step 4: Bonus - Deep Learning Model**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenize text
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences
max_len = 100
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)

# Build model
model_nn = Sequential([
    Dense(128, activation='relu', input_shape=(max_len,)),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train model
history = model_nn.fit(X_train_pad, y_train,
                      epochs=10,
                      batch_size=32,
                      validation_data=(X_test_pad, y_test))

# Evaluate
loss, accuracy = model_nn.evaluate(X_test_pad, y_test)
print(f"Neural Network Accuracy: {accuracy:.4f}")

"""# **Demo Script For Test Sentiment**"""

import joblib

# Load the saved model
model = joblib.load('sentiment_model.pkl')

def clean_text(text):
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    # Remove punctuation and numbers
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Convert to lowercase
    text = text.lower()
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords and lemmatize
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

def predict_sentiment(text):
    # Clean the input text
    cleaned_text = clean_text(text)
    # Make prediction
    prediction = model.predict([cleaned_text])
    sentiment = 'Positive' if prediction[0] == 1 else 'Negative'
    probability = model.predict_proba([cleaned_text])[0][prediction[0]]
    return sentiment, probability

# Get user input
print("Movie Review Sentiment Analyzer")
print("-------------------------------")
user_review = input("Please enter your movie review: ")

# Predict and display results
if user_review.strip():  # Check if input is not empty
    sentiment, confidence = predict_sentiment(user_review)
    print("\nAnalysis Results:")
    print(f"Sentiment: {sentiment}")
    print(f"Confidence: {confidence:.2%}")
    print("\nInterpretation:")
    if sentiment == 'Positive':
        print("This review expresses positive sentiment about the movie.")
    else:
        print("This review expresses negative sentiment about the movie.")
else:
    print("Error: Please enter a valid movie review.")